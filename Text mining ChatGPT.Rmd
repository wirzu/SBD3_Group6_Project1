---
title: "Text mining ChatGPT"
author: "Abishan, Josua, Lars, Luca"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

This research aims to explore and gain insights into Chat GPT, an advanced language model, through text mining of tweets. The dataset provided for this study consists of tweets collected between November 30, 2022, and January 29, 2023. With nearly 180,000 tweets at our disposal, a comprehensive analysis will be conducted to uncover valuable information.

The initial phase of this study involves a general analysis of the dataset, where patterns, trends, and key features will be identified. Following that, dictionaries will be defined, and automated content analysis techniques will be applied. This step will enable us to categorize and extract meaningful information from the tweets efficiently.

In the final stage, a sentiment analysis will be performed using the "syuzhet" method. This approach will allow us to assess the emotional tone and sentiment expressed in the tweets, providing a deeper understanding of the public perception and attitudes towards Chat GPT during the specified time period.

By leveraging the power of text mining and analyzing a substantial amount of Twitter data, this research aims to contribute to the existing knowledge about Chat GPT and provide valuable insights into its reception among Twitter users.

## Load packages and data

```{R}
library (syuzhet)
library (stringr)
library (tidyverse)
library (ggplot2)
library(scales)
library(stringi)
library(lubridate)
library(dplyr)
options(scipen=999)

load("ChatGPT.rda")
```

## 1. Question: What can you tell us about the users that tweet about ChatGPT?

```{R}
# Creating a copy of tweets
tweets_orig <- tweets

# take unique users
Users <- tweets[4:10]
Users = Users[!duplicated(Users$User),]

# Calculating average lenght of tweet
char_counts <- nchar(tweets$Tweet)
av_char_count <- mean(char_counts)
rounded_avg_char_count <- round(av_char_count, 2)

#create median
retweets_median = median(Users$Retweets)
retweets_mean = mean(Users$Retweets)

likes_median = median(Users$Likes)
likes_mean = mean(Users$Likes)

Friends_median = median(Users$UserFriends)
Friends_mean = mean(Users$UserFriends)

Followers_median = median(Users$UserFollowers)
Followers_mean = mean(Users$UserFollowers)

verified_median = median(Users$UserVerified)
verified_mean = mean(Users$UserVerified)

# Create a tibble with the values
my_table <- tibble(
  Statistic = c("Retweets", "Likes", "Friends", "Followers", "Verified"),
  Median = c(retweets_median, likes_median, Friends_median, Followers_median, verified_median),
  Average = c(retweets_mean, likes_mean, Friends_mean, Followers_mean, verified_mean)
)

print(my_table)
```
In the initial step, our focus was on analyzing the users who engaged with Chat GPT on Twitter. To ensure the accuracy of our analysis, we began by selecting unique user data, as there was a possibility that the same user may have tweeted about Chat GPT multiple times.

The resulting table provided us with valuable statistics on user engagement. By analyzing these user metrics, we gain insights into the reach, influence, and engagement levels of users discussing Chat GPT on Twitter. This information sets the foundation for a deeper understanding of user behavior and its potential impact on the perception and adoption of Chat GPT.


```{R}
# Count the number of tweets per user
tweet_counts <- as.data.frame(table(tweets$User))

# Rename the columns of the tweet_counts data frame
colnames(tweet_counts) <- c("User", "Tweet_Count")

# Create a new data frame with the number of tweets per user
multi_tweet_users <- tweet_counts[tweet_counts$Tweet_Count >= 1,]

# Create a table that shows the number of times each user has tweeted about Chat GPT
multi_tweet_table <- as.data.frame(table(multi_tweet_users$Tweet_Count))

# Rename the columns of the multi_tweet_table data frame
colnames(multi_tweet_table) <- c("Number_of_Tweets", "Number_of_Users")

# Print the multi_tweet_table data frame
multi_tweet_table
```
The generated table reveals that the majority of users only tweeted once about the topic. Surprisingly, only 19.64% of individuals tweeted a second time about Chat GPT. However, upon closer examination at the end of the table, it becomes evident that there are a few users who tweeted about it frequently. One particular user stands out, having tweeted 1325 times about Chat GPT within a span of 61 days, which is quite remarkable.

The frequency at which this person tweeted raises eyebrows. It implies that, on average, they posted a tweet every 66 minutes. Such prolific tweeting activity prompts curiosity and warrants further investigation into the motivations and circumstances behind their extensive engagement with Chat GPT.

This finding highlights the wide variation in user behavior when it comes to discussing Chat GPT on Twitter. While the majority of users exhibit minimal involvement, a small group demonstrates extraordinary dedication and interest in the topic. 

```{R}
# Create a table with the top 10 users with the highest tweet counts
top_10_users <- tweet_counts %>%
  arrange(desc(Tweet_Count)) %>%
  head(10)

# Print the top 10 users table
print(top_10_users)
```
We manually examined the users in the dataset, and it was observed that "MidJourneyAI_" had the highest number of tweets. However, upon closer inspection, it became apparent that this user had been repeatedly posting the same tweet using a bot. This behavior raises questions about the authenticity and motives behind their actions.

In contrast, the user with the second-highest number of tweets, "richardkimphd," appears to be a genuine account. This user predominantly shares tweets related to AI, indicating a keen interest in the topic. Their account reflects a high level of activity, suggesting that they are an active and engaged Twitter user.

These findings underscore the importance of discerning between automated or inauthentic accounts and genuine user engagement when analyzing Twitter data. Understanding the characteristics and behaviors of different users can help shed light on the broader dynamics and perspectives within the online AI community.

We decided to delete all multiple tweets before analyzing the data further.


```{R}
# Number of rows before removing duplicate tweets
before <- nrow(tweets)

# Remove rows with duplicate tweets
tweets_unique <- tweets[!duplicated(tweets$Tweet), ]

# Number of rows after removing duplicate tweets
after <- nrow(tweets_unique)

# Number of rows removed
removed <- before - after

# Display the number of removed rows
cat("The removal of duplicate tweets resulted in", removed, "rows being removed.")

```
Fortunately, only 959 tweets were removed, which is not a significant loss considering the dataset size of nearly 180,000 tweets.

The removal of these tweets has minimal impact on the overall analysis and findings. With such a vast collection of tweets, the remaining dataset still provides ample data for conducting meaningful analyses and drawing reliable conclusions.


```{R}
#Average/ median of Hour when to tweet, Nr. of Retweets, Likes, Followers, Friends, verified

#create Histogramm for Tweettime
plot_dataHour <- tweets %>% 
  group_by (timeofday_hour) %>%
  count()

ggplot (plot_dataHour, 
        aes (x=timeofday_hour, y=n)) +
  geom_bar(stat = "identity")+
  theme_minimal () +
  ggtitle("Number of tweets over time (per hour)") +
  xlab("Hour") +
  ylab("Number of tweets")
```
It is evident from the plot that the number of tweets steadily increases from around noon and peaks at around 5 PM. This pattern suggests that most people engage in tweeting about the topic during their work hours.

The surge in tweet activity during the afternoon hours aligns with the assumption that individuals are actively discussing and sharing their thoughts on Chat GPT while they are at work or engaged in related activities.

However, starting from 6 PM, there is a noticeable decline in the number of tweets. This decrease could be attributed to the assumption that individuals tend to finish their workday around 5 PM and gradually shift their focus to other activities or responsibilities outside of work.

```{R}
#Number of follower
#range breaks
range_breaks <- c(0, 100, 500, 1500, 5000, 15000000)

#Appling cut() on follower-data
Users$range <- cut(Users$UserFollowers, breaks = range_breaks)

Users <- na.omit(Users)

# Creating Barplot
ggplot(Users, aes(x = range, fill = range)) + 
  geom_bar() + 
  labs(title = "Number of Twitter-Follower", x = "Range", y = "Number of users") + scale_x_discrete(labels = c("0-100", "101-500", "501-1500", "1501-5000", "5000+")) +
  scale_y_continuous(labels = scales::comma_format()) +
  guides(fill = "none")

```
Hier Erklärung Grafik einfügen. (Joshi)

```{R}
#Number of tweets over time
plot_data <- tweets %>% 
  group_by (tweet_date) %>%
  count()

ggplot (plot_data, 
        aes (x=tweet_date, y=n)) +
  geom_bar(stat = "identity")+
  theme_minimal () +
  ggtitle("Number of tweets over time (per day)") +
  xlab("Date 22/23") +
  ylab("Number of tweets")
```
Hier Erklärung Grafik einfügen. (Joshi)
Es ist ein klarer Anstieg von Tweets zuerkennen vom 2. Dezember bis zum 9. Dezember. (noch übersetzten und erklären)

Ideen Abishan (muss noch kontrolliert werden)
OpenAI has unveiled on the 3th of November the public beta launch of the DALL·E API, providing developers with the opportunity to incorporate DALL·E's image generation capabilities into their own applications and products. With DALL·E's remarkable adaptability, users can effortlessly produce and modify a diverse range of images, spanning from artistic renditions to convincingly lifelike representations.

We suspect that the reason for the relative calm on Twitter in early December was likely due to the public beta release of the DALL·E API. It is possible that there were significant discussions and debates about Chat GPT during the period from early November to early December, which gradually subsided. However, on December 8, OpenAI published a new piece of content that once again sparked discussions surrounding Chat GPT.

On December 8, OpenAI released an interview featuring Christian Gibson, an engineer on the Supercomputing team at the company. In the interview, he shares insights into his journey into engineering and how he became a part of OpenAI. Christian also discusses the specific challenges he focuses on addressing, such as the complexities involved in AI workflows and overcoming bottlenecks when running codes on supercomputers. He highlights what sets working on supercomputing at OpenAI apart from other places, such as the immense scale of the operation, and provides a glimpse into his typical day at OpenAI.

This practical and informative publication likely led to renewed discussions and conversations about Chat GPT.


```{R}
# Convert UserCreated to datetime format
Users$UserCreated <- ymd_hms(Users$UserCreated)

# Create the plot
ggplot(Users, aes(x = UserCreated)) + 
  geom_histogram(bins = 50, fill = "#69b3a2", color = "#e9ecef") +
  labs(x = "Account Creation Date", y = "Number of Accounts") +
  ggtitle("Twitter Account Creation Dates")

```
Hier Erklärung Grafik einfügen. (Joshi)

## 2. What are the tweets about, what do users associated the new technology with (e.g. industries, specific applications, and also emotions)?

## Pre processing

```{R}
# Define a function to preprocess the text
preprocess_text <- function(text) {
  
  # Convert text to lower case
  text <- tolower(text)
  
  # Remove emojis and emoticons
  text <- gsub("[\U0001F600-\U0001F64F\U0001F910-\U0001F96F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]", "", text, perl=TRUE)
  
  # Remove numbers
  text <- gsub("\\d+", "", text)
  
  # Remove punctuation
  text <- gsub("[[:punct:]]", "", text)
  
  # Remove whitespace
  text <- gsub("\\s+", " ", text)
  
  # Remove stopwords and other words to be removed
  words_to_remove <- c("the", "and", "in", "to", "a", "of")
  words_to_remove_pattern <- paste0("\\b(", paste(words_to_remove, collapse = "|"), ")\\b")
  text <- gsub(words_to_remove_pattern, "", text, ignore.case = TRUE)
  
  # Return the preprocessed text
  return(text)
}

# Apply the preprocessing function to the Tweet column
tweets$preprocessed_text <- sapply(tweets$Tweet, preprocess_text)

```
Erklärung zu pre Prozessing, bzw. was wir entfernt haben. (Absichtlich nicht buchstaben sonder Füllwörter entfernt)
```{R}

### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 3: PERFORM AUTOMATED CONTENT ANALYSIS
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

#Select text column and create your custom dictionary
# dic1 = industries evtl. noch genauer beispiel Bildungswesen
#tweets$dic1 <- "NA"
tweets$dic1 <-str_count(tweets$preprocessed_text, "artificial intelligence|machine learning|automation|robotics|data")
#tweets$dic1_occurence<- "NA"
tweets$dic1_occurence<-ifelse(tweets$dic1>=2,1,0)
# dic2 = specific applications
tweets$dic2 <-str_count(tweets$preprocessed_text, "chatbot|language modeling|ai|artificial intelligence|ml|machine learning")
tweets$dic2_occurence<-ifelse(tweets$dic2>=2,1,0)
# dic3 = emotions
tweets$dic3 <-str_count(tweets$preprocessed_text, "excited|happy|frustated|angry|sad|amused")
tweets$dic3_occurence<-ifelse(tweets$dic3>=2,1,0)
# dic4 = hype
tweets$dic4 <-str_count(tweets$preprocessed_text, "excited|hyped|thrilled|stoked|pumped")
tweets$dic4_occurence<-ifelse(tweets$dic4>=2,1,0)
# dic5 = consulting company, digital transformation
tweets$dic5 <-str_count(tweets$preprocessed_text, "analysis|big data|classification|neural networks|summarization")
tweets$dic5_occurence<-ifelse(tweets$dic5>=2,1,0)

sum (tweets$dic1_occurence)
sum(tweets$dic2_occurence)
sum(tweets$dic3_occurence)
sum(tweets$dic4_occurence)
sum(tweets$dic5_occurence)
```

```{R}
## VISUALIZE RESULTS dictionary 1 (industries)
#sum of reviews that cover topic per day
plot_content_data1 <- tweets %>% 
  group_by (tweet_date) %>%
  summarise(n_content=sum(dic1_occurence))

ggplot (plot_content_data1, aes (x=tweet_date, y=n_content)) + geom_bar(stat = "identity")+ theme_minimal () + ggtitle("Number of reviews covering the topic industries (per month)")
```
Erklärung einfügen Dictionary industries!!!
```{R}
## VISUALIZE RESULTS dictionary 2 (specific applications)
#sum of reviews that cover topic per day
plot_content_data2 <- tweets %>% 
  group_by (tweet_date) %>%
  summarise(n_content=sum(dic2_occurence))

ggplot (plot_content_data2, aes (x=tweet_date, y=n_content)) + geom_bar(stat = "identity")+ theme_minimal () + ggtitle("Number of reviews covering the topic specific applications (per month)")
```
Erklärung einfügen Dictionary specific applications!!!
```{R}
## VISUALIZE RESULTS dictionary 3 (emotions)
#sum of reviews that cover topic per day
plot_content_data3 <- tweets %>% 
  group_by (tweet_date) %>%
  summarise(n_content=sum(dic3_occurence))

ggplot (plot_content_data3, aes (x=tweet_date, y=n_content)) + geom_bar(stat = "identity")+ theme_minimal () + ggtitle("Number of reviews covering the topic emotions (per month)")
```
Erklärung einfügen Dictionary emotions!!!
```{R}
## VISUALIZE RESULTS dictionary 4 (hype)
#sum of reviews that cover topic per day
plot_content_data4 <- tweets %>% 
  group_by (tweet_date) %>%
  summarise(n_content=sum(dic4_occurence))

ggplot (plot_content_data4, aes (x=tweet_date, y=n_content)) + geom_bar(stat = "identity")+ theme_minimal () + ggtitle("Number of reviews covering the topic hype (per month)")
```
Erklärung einfügen Dictionary hype!!!
```{R}
# Perform sentiment analysis
#Select text column and calculate sentiment scores. You can change the method (e.g."syuzhet", "bing", "nrc")
tweets$sentiment <- "NA"
tweets$sentiment <- get_sentiment(tweets$preprocessed_text, method="syuzhet", lang="english")

## VISUALIZE RESULTS
# mean over time
plot_sentiment_data <- tweets %>% 
  group_by (tweet_date) %>%
  summarise(n_sentiment=mean(sentiment))

ggplot (plot_sentiment_data, aes (x=tweet_date, y=n_sentiment)) + geom_line()+ theme_minimal () + ggtitle("Sentiment scores over time (mean per month)")
```

```{R}

### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### STEP 5: PERFORM ADDITIONAL ANALYSIS
### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

#correlation of sentiment with custom dictionary
cor(tweets$dic1, tweets$Likes, method = "pearson")
cor(tweets$dic1, tweets$sentiment, method = "pearson")
cor(tweets$dic2, tweets$Likes, method = "pearson")
cor(tweets$dic2, tweets$sentiment, method = "pearson")
cor(tweets$dic3, tweets$Likes, method = "pearson")
cor(tweets$dic3, tweets$sentiment, method = "pearson")
cor(tweets$dic4, tweets$Likes, method = "pearson")
cor(tweets$dic4, tweets$sentiment, method = "pearson")
````